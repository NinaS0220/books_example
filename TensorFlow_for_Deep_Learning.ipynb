{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eff9ca7",
   "metadata": {},
   "source": [
    "## Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16527b0a",
   "metadata": {},
   "source": [
    "Инициализация интерактвиного сеанса Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f6aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e1964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from version v2.x you should use this for activate version\n",
    "import tensorflow.compat.v1 as tfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df7caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tfc.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06958b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание тензора нулей\n",
    "a = tf.zeros(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aea118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# возрат значения\n",
    "print(a.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92a856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# выведение матрицы с единицами\n",
    "b = tf.ones((2, 2, 2))\n",
    "b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a2121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполнение тензоров произвольными значениями\n",
    "c = tf.fill((2, 2), value=5.)\n",
    "c.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecce0330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание константных тензоров (не должны изменяться во время выполнения программ)\n",
    "a = tf.constant(3)\n",
    "a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7993ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# отбор случайных значений для тензора из нормального распределения\n",
    "b = tf.random.normal((2, 2), mean=0, stddev=1)\n",
    "b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840f742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# отбор случайных значений для тензора из равномерного распределения\n",
    "a = tf.random.uniform((2, 2), minval=-2, maxval=2)\n",
    "a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4e30dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сложение тензоров\n",
    "c = tf.ones((2, 2))\n",
    "d = tf.ones((2, 2))\n",
    "e = c + d\n",
    "e.numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a018b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 2 * e\n",
    "f.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c56a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# поэлементное умножение (при умножении тензоров мы получаем не матричное умножение, а поэлементное)\n",
    "c = tf.fill((2, 2), 2.)\n",
    "d = tf.fill((2, 2), 7.)\n",
    "e = c * d\n",
    "e.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650884d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание единичной матрицы - это квадратная матрица, элементы которой равны 0 везде, кроме главной диагонали, где они равны 1\n",
    "tf.eye(4).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0475e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.range(начало, граница(не включительно), дельта)\n",
    "r = tf.range(1, 5, 1)\n",
    "r.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cedc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# диагональная матрица - так же являются ненулевыми по диагонали, но по диагонали могут быть произвольные значения\n",
    "d = tf.linalg.diag(r)\n",
    "d.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d74123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# получение транспонированной матрицы\n",
    "a = tf.ones((2, 3))\n",
    "a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "at = tf.transpose(a)\n",
    "at.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9e8b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# выполнение матричного умножения\n",
    "b = tf.ones((3, 4))\n",
    "c = tf.matmul(a, b)\n",
    "c.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b75feed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# смена типа данных тензора\n",
    "a = tf.ones((2, 2), dtype=tf.int32)\n",
    "a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1f1e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.cast(a, tf.float32)\n",
    "b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb01124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# манипуляция с формами тензоров. reshape - позволяет конвертировать тензоры в тензоры другой формы\n",
    "a = tf.ones(8)\n",
    "a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b77bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.reshape(a, (4, 2))\n",
    "b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49228fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.reshape(a, (2, 2, 2))\n",
    "c.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4cdffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# получение формы тензора\n",
    "a = tf.ones(2)\n",
    "a.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e57b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand_dims добавляет в тензор новую размерность размера 1 \n",
    "b = tf.expand_dims(a, 0)\n",
    "b.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e23dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56f7fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.expand_dims(a, 1)\n",
    "c.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f92b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d6902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# squeeze удаляет из тензора все размерности размера 1, превращая например векторк-строку 2 ранга в вектор 1-го ранга\n",
    "d = tf.squeeze(b)\n",
    "d.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d46789",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc1633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# примеры транслирования - добавления к матрицам тензорной системы вектора разного размера\n",
    "a = tf.ones((2, 2))\n",
    "b = tf.range(0, 2, 1, dtype=tf.float32) \n",
    "# если не задать явно тип, то будет ошибка, т.к. tensorflow не выполняет неявное приведение типов\n",
    "b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f551e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a + b\n",
    "c.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a293c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow выполняет только декларативное сложение\n",
    "a = tf.constant(3)\n",
    "b = tf.constant(4)\n",
    "c = a + b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beac03a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# обработка сеансов явным образом\n",
    "tf.compat.v1.disable_eager_execution() # need to disable eager in TF2.x\n",
    "sess = tfc.Session()\n",
    "a = tf.ones((2, 2))\n",
    "b = tf.matmul(a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c4e9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bcd2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# объекты-переменные Variable служат контейнером для тензоров\n",
    "a = tf.Variable(tf.ones((2, 2)))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a047abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.global_variables_initializer() - инициализирует все переменные для расчета. более не используется\n",
    "init_op = tf.local_variables_initializer()\n",
    "sess.run(init_op)\n",
    "print(sess.run(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e84a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# при помощи функции assign мы можем обновить значение существующей переменной\n",
    "tf.compat.v1.disable_eager_execution() # need to disable eager in TF2.x\n",
    "sess = tfc.Session()\n",
    "sess.run(a.assign(tf.zeros((2, 2))))\n",
    "sess.run(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676b5882",
   "metadata": {},
   "source": [
    "## Chapter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b9f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# игрушечный набор регрессионных данных\n",
    "import numpy as np\n",
    "np.random.seed(456)\n",
    "import  tensorflow as tf\n",
    "tf.random.set_seed(456)\n",
    "from matplotlib import rc\n",
    "rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23767f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7bf645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функции ошибок\n",
    "def pearson_r2_score(y, y_pred):\n",
    " # \"\"\"Computes Pearson R^2 (square of Pearson correlation).\"\"\"\n",
    "  return pearsonr(y, y_pred)[0]**2\n",
    "\n",
    "def rms_score(y_true, y_pred):\n",
    "# \"\"\"Computes RMS error.\"\"\"\n",
    "  return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1204a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "N = 3\n",
    "#N = 100\n",
    "w_true = 5\n",
    "b_true = 2\n",
    "noise_scale = .1\n",
    "x_np = np.random.rand(N, 1)\n",
    "noise = np.random.normal(scale=noise_scale, size=(N, 1))\n",
    "# Convert shape of y_np to (N,)\n",
    "y_np = np.reshape(w_true * x_np  + b_true + noise, (-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b894fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save image of the data distribution\n",
    "plt.scatter(x_np, y_np);\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim(0, 1)\n",
    "plt.title(\"Toy Linear Regression Data, $y = 5x + 2 + N(0, 1)$\")\n",
    "plt.savefig(\"lr_data.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df97c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# тренировка линейной регрессионной модели\n",
    "# Generate tensorflow graph\n",
    "# placeholders - (заполнитель) это способ ввода информации в вычислительный граф\n",
    "\n",
    "import tensorflow.compat.v1 as tfc\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('placeholders'):\n",
    "        x = tfc.placeholder(tf.float32, (3, 1))\n",
    "        y = tfc.placeholder(tf.float32, (3, ))\n",
    "# name_scope (область имен) - представляет собой механизм определения областей видимости для управления коллекциями переменных\n",
    "# обратите внимание, что x - это скаляр, поэтому W - это одиночный заучиваемый вес\n",
    "    with tf.name_scope('weights'):\n",
    "        W = tf.Variable(tfc.random_normal((1, 1)))\n",
    "        b = tf.Variable(tfc.random_normal((1,)))\n",
    "    with tf.name_scope('prediction'):\n",
    "        y_pred = tf.matmul(x,W) + b\n",
    "    with tf.name_scope('loss'):\n",
    "        l = tf.reduce_sum((y-y_pred)**2)\n",
    "    #Добавить оптимизацию тренировки\n",
    "    with tf.name_scope('optim'):\n",
    "        #Задать скорость заучивания .001, как рекомендовано выше.\n",
    "        train_op = tfc.train.AdamOptimizer(.001).minimize(l)\n",
    "    with tf.name_scope('summaries'):\n",
    "        #Запись сводки о переменных(скалярных величинах) в заданный каталог журналов\n",
    "        # добавление сводного отчета для функции потерь\n",
    "        tfc.summary.scalar('loss', l)\n",
    "        #Объединение нескольких сводок в одну\n",
    "        merged = tfc.summary.merge_all()\n",
    "\n",
    "    train_writer = tfc.summary.FileWriter('/tmp/lr-train', graph)\n",
    "\n",
    "    n_steps = 8000\n",
    "\n",
    "    with tfc.Session() as sess:\n",
    "        sess.run(tfc.global_variables_initializer())\n",
    "        \n",
    "        #Натренировать модель\n",
    "        for i in range(n_steps):\n",
    "            feed_dict1 = {x: [[1.], [2.], [3.]], y: [2., 3., 4.]}\n",
    "            _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict1)\n",
    "            train_writer.add_summary(summary, i)\n",
    "\n",
    "            # print epoch and loss\n",
    "            if i % 100 == 0:\n",
    "                print(f'Epoch: {i}'.ljust(13) + f'loss: {loss:.4f}'.ljust(16))\n",
    "            \n",
    "              # Get weights\n",
    "            w_final, b_final = sess.run([W, b])\n",
    "\n",
    "              # Make Predictions\n",
    "            y_pred_np = sess.run(y_pred, feed_dict=feed_dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a46966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_np = np.reshape(y_pred_np, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25e343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# квадратичный коэффициент корреляции Пирсона  - это мера корреляции между двумя переменными, которая принимает значение от +1 до 0\n",
    "# при этом +1 указывает на идеальную корреляцию, а 0 - на отсутствие корреляции\n",
    "y_pred_np = np.reshape(y_pred_np, -1)\n",
    "r2 = pearson_r2_score(y_np, y_pred_np)\n",
    "print(\"Pearson R^2: %f\" % r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fb0d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE - является мерой усредненной разницы между предсказанными значениями и истинными\n",
    "rms = rms_score(y_np, y_pred_np)\n",
    "print(\"RMS: %f\" % rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35539b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clear figure\n",
    "plt.clf()\n",
    "plt.xlabel(\"Y-true\")\n",
    "plt.ylabel(\"Y-pred\")\n",
    "plt.title(\"Predicted versus True values \"\n",
    "          r\"(Pearson $R^2$: $0.994$)\")\n",
    "plt.scatter(y_np, y_pred_np)\n",
    "plt.savefig(\"lr_pred.png\")\n",
    "\n",
    "# Now draw with learned regression line\n",
    "plt.clf()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"True Model versus Learned Model \"\n",
    "          r\"(RMS: $1.027620$)\")\n",
    "plt.xlim(0, 1)\n",
    "plt.scatter(x_np, y_np)\n",
    "x_left = 0\n",
    "y_left = w_final[0]*x_left + b_final\n",
    "x_right = 1\n",
    "y_right = w_final[0]*x_right + b_final\n",
    "plt.plot([x_left, x_right], [y_left, y_right], color='k')\n",
    "plt.savefig(\"lr_learned.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56749bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример логистической регрессии\n",
    "import numpy as np\n",
    "np.random.seed(456)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(456)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import logit\n",
    "\n",
    "# Generate synthetic data\n",
    "N = 100\n",
    "# Zeros form a Gaussian centered at (-1, -1)\n",
    "x_zeros = np.random.multivariate_normal(\n",
    "    mean=np.array((-1, -1)), cov=.1*np.eye(2), size=(N//2,))\n",
    "y_zeros = np.zeros((N//2,))\n",
    "# Ones form a Gaussian centered at (1, 1)\n",
    "x_ones = np.random.multivariate_normal(\n",
    "    mean=np.array((1, 1)), cov=.1*np.eye(2), size=(N//2,))\n",
    "y_ones = np.ones((N//2,))\n",
    "\n",
    "x_np = np.vstack([x_zeros, x_ones])\n",
    "y_np = np.concatenate([y_zeros, y_ones])\n",
    "\n",
    "# Save image of the data distribution\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "plt.title(\"Toy Logistic Regression Data\")\n",
    "\n",
    "# Plot Zeros\n",
    "plt.scatter(x_zeros[:, 0], x_zeros[:, 1], color=\"blue\")\n",
    "plt.scatter(x_ones[:, 0], x_ones[:, 1], color=\"red\")\n",
    "plt.savefig(\"logistic_data.png\")\n",
    "\n",
    "# Generate tensorflow graph\n",
    "with tf.name_scope(\"placeholders\"):\n",
    "  x = tf.placeholder(tf.float32, (N, 2))\n",
    "  y = tf.placeholder(tf.float32, (N,))\n",
    "with tf.name_scope(\"weights\"):\n",
    "  W = tf.Variable(tf.random_normal((2, 1)))\n",
    "  b = tf.Variable(tf.random_normal((1,)))\n",
    "with tf.name_scope(\"prediction\"):\n",
    "  y_logit = tf.squeeze(tf.matmul(x, W) + b)\n",
    "  # the sigmoid gives the class probability of 1\n",
    "  y_one_prob = tf.sigmoid(y_logit)\n",
    "  # Rounding P(y=1) will give the correct prediction.\n",
    "  y_pred = tf.round(y_one_prob)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "  # Compute the cross-entropy term for each datapoint\n",
    "  entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y)\n",
    "  # Sum all contributions\n",
    "  l = tf.reduce_sum(entropy)\n",
    "with tf.name_scope(\"optim\"):\n",
    "  train_op = tf.train.AdamOptimizer(.01).minimize(l)\n",
    "\n",
    "with tf.name_scope(\"summaries\"):\n",
    "  tf.summary.scalar(\"loss\", l)\n",
    "  merged = tf.summary.merge_all()\n",
    "\n",
    "train_writer = tf.summary.FileWriter('/tmp/logistic-train', tf.get_default_graph())\n",
    "\n",
    "n_steps = 1000\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  # Train model\n",
    "  for i in range(n_steps):\n",
    "    feed_dict = {x: x_np, y: y_np}\n",
    "    _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n",
    "    print(\"loss: %f\" % loss)\n",
    "    train_writer.add_summary(summary, i)\n",
    "\n",
    "  # Get weights\n",
    "  w_final, b_final = sess.run([W, b])\n",
    "\n",
    "  # Make Predictions\n",
    "  y_pred_np = sess.run(y_pred, feed_dict={x: x_np})\n",
    "\n",
    "score = accuracy_score(y_np, y_pred_np)\n",
    "print(\"Classification Accuracy: %f\" % score)\n",
    "\n",
    "plt.clf()\n",
    "# Save image of the data distribution\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "plt.title(\"Learned Model (Classification Accuracy: 1.00)\")\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(-2, 2)\n",
    "\n",
    "# Plot Zeros\n",
    "plt.scatter(x_zeros[:, 0], x_zeros[:, 1], color=\"blue\")\n",
    "plt.scatter(x_ones[:, 0], x_ones[:, 1], color=\"red\")\n",
    "\n",
    "x_left = -2\n",
    "y_left = (1./w_final[1]) * (-b_final + logit(.5) - w_final[0]*x_left)\n",
    "\n",
    "x_right = 2\n",
    "y_right = (1./w_final[1]) * (-b_final + logit(.5) - w_final[0]*x_right)\n",
    "plt.plot([x_left, x_right], [y_left, y_right], color='k')\n",
    "\n",
    "plt.savefig(\"logistic_pred.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421be5c7",
   "metadata": {},
   "source": [
    "## Chapter 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124da57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# обработка набора с токсичными данными Tox21 и использования скрытого слоя с отсевом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb7d44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(456)\n",
    "import  tensorflow as tf\n",
    "tf.set_random_seed(456)\n",
    "import matplotlib.pyplot as plt\n",
    "import deepchem as dc\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "_, (train, valid, test), _ = dc.molnet.load_tox21()\n",
    "train_X, train_y, train_w = train.X, train.y, train.w\n",
    "valid_X, valid_y, valid_w = valid.X, valid.y, valid.w\n",
    "test_X, test_y, test_w = test.X, test.y, test.w\n",
    "\n",
    "# Remove extra tasks\n",
    "train_y = train_y[:, 0]\n",
    "valid_y = valid_y[:, 0]\n",
    "test_y = test_y[:, 0]\n",
    "train_w = train_w[:, 0]\n",
    "valid_w = valid_w[:, 0]\n",
    "test_w = test_w[:, 0]\n",
    "\n",
    "\n",
    "# Generate tensorflow graph\n",
    "d = 1024\n",
    "n_hidden = 50\n",
    "learning_rate = .001\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "dropout_prob = 1.0\n",
    "\n",
    "with tf.name_scope(\"placeholders\"):\n",
    "  x = tf.placeholder(tf.float32, (None, d))\n",
    "  y = tf.placeholder(tf.float32, (None,))\n",
    "  keep_prob = tf.placeholder(tf.float32)\n",
    "with tf.name_scope(\"hidden-layer\"):\n",
    "  W = tf.Variable(tf.random_normal((d, n_hidden)))\n",
    "  b = tf.Variable(tf.random_normal((n_hidden,)))\n",
    "  x_hidden = tf.nn.relu(tf.matmul(x, W) + b)\n",
    "  # Apply dropout\n",
    "  x_hidden = tf.nn.dropout(x_hidden, keep_prob)\n",
    "with tf.name_scope(\"output\"):\n",
    "  W = tf.Variable(tf.random_normal((n_hidden, 1)))\n",
    "  b = tf.Variable(tf.random_normal((1,)))\n",
    "  y_logit = tf.matmul(x_hidden, W) + b\n",
    "  # the sigmoid gives the class probability of 1\n",
    "  y_one_prob = tf.sigmoid(y_logit)\n",
    "  # Rounding P(y=1) will give the correct prediction.\n",
    "  y_pred = tf.round(y_one_prob)\n",
    "with tf.name_scope(\"loss\"):\n",
    "  # Compute the cross-entropy term for each datapoint\n",
    "  y_expand = tf.expand_dims(y, 1)\n",
    "  entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y_expand)\n",
    "  # Sum all contributions\n",
    "  l = tf.reduce_sum(entropy)\n",
    "\n",
    "with tf.name_scope(\"optim\"):\n",
    "  train_op = tf.train.AdamOptimizer(learning_rate).minimize(l)\n",
    "\n",
    "with tf.name_scope(\"summaries\"):\n",
    "  tf.summary.scalar(\"loss\", l)\n",
    "  merged = tf.summary.merge_all()\n",
    "\n",
    "train_writer = tf.summary.FileWriter('/tmp/fcnet-tox21-dropout',\n",
    "                                     tf.get_default_graph())\n",
    "N = train_X.shape[0]\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  step = 0\n",
    "  for epoch in range(n_epochs):\n",
    "    pos = 0\n",
    "    while pos < N:\n",
    "      batch_X = train_X[pos:pos+batch_size]\n",
    "      batch_y = train_y[pos:pos+batch_size]\n",
    "      feed_dict = {x: batch_X, y: batch_y, keep_prob: dropout_prob}\n",
    "      _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n",
    "      print(\"epoch %d, step %d, loss: %f\" % (epoch, step, loss))\n",
    "      train_writer.add_summary(summary, step)\n",
    "    \n",
    "      step += 1\n",
    "      pos += batch_size\n",
    "\n",
    "  # Make Predictions (set keep_prob to 1.0 for predictions)\n",
    "  train_y_pred = sess.run(y_pred, feed_dict={x: train_X, keep_prob: 1.0})\n",
    "  valid_y_pred = sess.run(y_pred, feed_dict={x: valid_X, keep_prob: 1.0})\n",
    "  test_y_pred = sess.run(y_pred, feed_dict={x: test_X, keep_prob: 1.0})\n",
    "\n",
    "train_weighted_score = accuracy_score(train_y, train_y_pred, sample_weight=train_w)\n",
    "print(\"Train Weighted Classification Accuracy: %f\" % train_weighted_score)\n",
    "valid_weighted_score = accuracy_score(valid_y, valid_y_pred, sample_weight=valid_w)\n",
    "print(\"Valid Weighted Classification Accuracy: %f\" % valid_weighted_score)\n",
    "test_weighted_score = accuracy_score(test_y, test_y_pred, sample_weight=test_w)\n",
    "print(\"Test Weighted Classification Accuracy: %f\" % test_weighted_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83433b04",
   "metadata": {},
   "source": [
    "## Chapter 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa17f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# определение и тренировка случайного леса на наборе данных Tox21\n",
    "import numpy as np\n",
    "np.random.seed(456)\n",
    "import matplotlib.pyplot as plt\n",
    "import deepchem as dc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "_, (train, valid, test), _ = dc.molnet.load_tox21()\n",
    "train_X, train_y, train_w = train.X, train.y, train.w\n",
    "valid_X, valid_y, valid_w = valid.X, valid.y, valid.w\n",
    "test_X, test_y, test_w = test.X, test.y, test.w\n",
    "\n",
    "# Remove extra tasks\n",
    "train_y = train_y[:, 0]\n",
    "valid_y = valid_y[:, 0]\n",
    "test_y = test_y[:, 0]\n",
    "train_w = train_w[:, 0]\n",
    "valid_w = valid_w[:, 0]\n",
    "test_w = test_w[:, 0]\n",
    "\n",
    "# Generate tensorflow graph\n",
    "sklearn_model = RandomForestClassifier(\n",
    "    class_weight=\"balanced\", n_estimators=50)\n",
    "print(\"About to fit model on train set.\")\n",
    "sklearn_model.fit(train_X, train_y)\n",
    "\n",
    "train_y_pred = sklearn_model.predict(train_X)\n",
    "valid_y_pred = sklearn_model.predict(valid_X)\n",
    "test_y_pred = sklearn_model.predict(test_X)\n",
    "\n",
    "weighted_score = accuracy_score(train_y, train_y_pred, sample_weight=train_w)\n",
    "print(\"Weighted train Classification Accuracy: %f\" % weighted_score)\n",
    "weighted_score = accuracy_score(valid_y, valid_y_pred, sample_weight=valid_w)\n",
    "print(\"Weighted valid Classification Accuracy: %f\" % weighted_score)\n",
    "weighted_score = accuracy_score(test_y, test_y_pred, sample_weight=test_w)\n",
    "print(\"Weighted test Classification Accuracy: %f\" % weighted_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ef536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# использование полносвязных сетей с гиперпараметрами \n",
    "import numpy as np\n",
    "np.random.seed(456)\n",
    "import  tensorflow as tf\n",
    "tf.set_random_seed(456)\n",
    "import matplotlib.pyplot as plt\n",
    "import deepchem as dc\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def eval_tox21_hyperparams(n_hidden=50, n_layers=1, learning_rate=.001,\n",
    "                           dropout_prob=0.5, n_epochs=45, batch_size=100,\n",
    "                           weight_positives=True):\n",
    "    \n",
    "# n_hidden - управляет количеством нейронов в каждом скрытом слое сети\n",
    "# n_layers - количество скрытых слоев\n",
    "# learning_rate - контролирует скорость заучивания\n",
    "# dropout_prob - является вероятностью, что нейроны не сброшены во время шагов тренировки\n",
    "# n_epochs - количество проходов по всем данным\n",
    "# batch_size - количество точек данных в каждом пакете\n",
    "# weight_positives - взвешивает прецеденты классов, чтобы они обладали одинаковым весом\n",
    "\n",
    "  print(\"---------------------------------------------\")\n",
    "  print(\"Model hyperparameters\")\n",
    "  print(\"n_hidden = %d\" % n_hidden)\n",
    "  print(\"n_layers = %d\" % n_layers)\n",
    "  print(\"learning_rate = %f\" % learning_rate)\n",
    "  print(\"n_epochs = %d\" % n_epochs)\n",
    "  print(\"batch_size = %d\" % batch_size)\n",
    "  print(\"weight_positives = %s\" % str(weight_positives))\n",
    "  print(\"dropout_prob = %f\" % dropout_prob)\n",
    "  print(\"---------------------------------------------\")\n",
    "\n",
    "  d = 1024\n",
    "  graph = tf.Graph()\n",
    "  with graph.as_default():\n",
    "    _, (train, valid, test), _ = dc.molnet.load_tox21()\n",
    "    train_X, train_y, train_w = train.X, train.y, train.w\n",
    "    valid_X, valid_y, valid_w = valid.X, valid.y, valid.w\n",
    "    test_X, test_y, test_w = test.X, test.y, test.w\n",
    "\n",
    "    # Remove extra tasks\n",
    "    train_y = train_y[:, 0]\n",
    "    valid_y = valid_y[:, 0]\n",
    "    test_y = test_y[:, 0]\n",
    "    train_w = train_w[:, 0]\n",
    "    valid_w = valid_w[:, 0]\n",
    "    test_w = test_w[:, 0]\n",
    "\n",
    "    # Generate tensorflow graph\n",
    "    with tf.name_scope(\"placeholders\"):\n",
    "      x = tf.placeholder(tf.float32, (None, d))\n",
    "      y = tf.placeholder(tf.float32, (None,))\n",
    "      w = tf.placeholder(tf.float32, (None,))\n",
    "      keep_prob = tf.placeholder(tf.float32)\n",
    "    for layer in range(n_layers):\n",
    "      with tf.name_scope(\"layer-%d\" % layer):\n",
    "        W = tf.Variable(tf.random_normal((d, n_hidden)))\n",
    "        b = tf.Variable(tf.random_normal((n_hidden,)))\n",
    "        x_hidden = tf.nn.relu(tf.matmul(x, W) + b)\n",
    "        # Apply dropout\n",
    "        x_hidden = tf.nn.dropout(x_hidden, keep_prob)\n",
    "    with tf.name_scope(\"output\"):\n",
    "      W = tf.Variable(tf.random_normal((n_hidden, 1)))\n",
    "      b = tf.Variable(tf.random_normal((1,)))\n",
    "      y_logit = tf.matmul(x_hidden, W) + b\n",
    "      # the sigmoid gives the class probability of 1\n",
    "      y_one_prob = tf.sigmoid(y_logit)\n",
    "      # Rounding P(y=1) will give the correct prediction.\n",
    "      y_pred = tf.round(y_one_prob)\n",
    "    with tf.name_scope(\"loss\"):\n",
    "      # Compute the cross-entropy term for each datapoint\n",
    "      y_expand = tf.expand_dims(y, 1)\n",
    "      entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y_expand)\n",
    "      # Multiply by weights\n",
    "      if weight_positives:\n",
    "        w_expand = tf.expand_dims(w, 1)\n",
    "        entropy = w_expand * entropy\n",
    "      # Sum all contributions\n",
    "      l = tf.reduce_sum(entropy)\n",
    "\n",
    "    with tf.name_scope(\"optim\"):\n",
    "      train_op = tf.train.AdamOptimizer(learning_rate).minimize(l)\n",
    "\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "      tf.summary.scalar(\"loss\", l)\n",
    "      merged = tf.summary.merge_all()\n",
    "\n",
    "    hyperparam_str = \"d-%d-hidden-%d-lr-%f-n_epochs-%d-batch_size-%d-weight_pos-%s\" % (\n",
    "        d, n_hidden, learning_rate, n_epochs, batch_size, str(weight_positives))\n",
    "    train_writer = tf.summary.FileWriter('/tmp/fcnet-func-' + hyperparam_str,\n",
    "                                         tf.get_default_graph())\n",
    "    N = train_X.shape[0]\n",
    "    with tf.Session() as sess:\n",
    "      sess.run(tf.global_variables_initializer())\n",
    "      step = 0\n",
    "      for epoch in range(n_epochs):\n",
    "        pos = 0\n",
    "        while pos < N:\n",
    "          batch_X = train_X[pos:pos+batch_size]\n",
    "          batch_y = train_y[pos:pos+batch_size]\n",
    "          batch_w = train_w[pos:pos+batch_size]\n",
    "          feed_dict = {x: batch_X, y: batch_y, w: batch_w, keep_prob: dropout_prob}\n",
    "          _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n",
    "          print(\"epoch %d, step %d, loss: %f\" % (epoch, step, loss))\n",
    "          train_writer.add_summary(summary, step)\n",
    "        \n",
    "          step += 1\n",
    "          pos += batch_size\n",
    "\n",
    "      # Make Predictions (set keep_prob to 1.0 for predictions)\n",
    "      valid_y_pred = sess.run(y_pred, feed_dict={x: valid_X, keep_prob: 1.0})\n",
    "\n",
    "    weighted_score = accuracy_score(valid_y, valid_y_pred, sample_weight=valid_w)\n",
    "    print(\"Valid Weighted Classification Accuracy: %f\" % weighted_score)\n",
    "  return weighted_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  score = eval_tox21_hyperparams()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2241b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# перебор гиперпараметров через цикл for\n",
    "\n",
    "import numpy as np\n",
    "from fcnet_func import eval_tox21_hyperparams\n",
    "\n",
    "scores = {}\n",
    "n_reps = 3 \n",
    "hidden_sizes = [30, 60]\n",
    "epochs = [15, 30, 45]\n",
    "dropouts = [.5]\n",
    "num_layers = [1, 2]\n",
    "\n",
    "for rep in range(n_reps):\n",
    "  for n_epochs in epochs:\n",
    "    for hidden_size in hidden_sizes:\n",
    "      for dropout in dropouts:\n",
    "        for n_layers in num_layers:\n",
    "          score = eval_tox21_hyperparams(n_hidden=hidden_size, n_epochs=n_epochs,\n",
    "                                         dropout_prob=dropout, n_layers=n_layers)\n",
    "          if (hidden_size, n_epochs, dropout, n_layers) not in scores:\n",
    "            scores[(hidden_size, n_epochs, dropout, n_layers)] = []\n",
    "          scores[(hidden_size, n_epochs, dropout, n_layers)].append(score)\n",
    "print(\"All Scores\")\n",
    "print(scores)\n",
    "\n",
    "avg_scores = {}\n",
    "for params, param_scores in scores.iteritems():\n",
    "  avg_scores[params] = np.mean(np.array(param_scores))\n",
    "print(\"Scores Averaged over %d repetitions\" % n_reps)\n",
    "print(avg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc5679",
   "metadata": {},
   "source": [
    "## Chapter 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09915fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# обработка данных MNIST с помощью CNN\n",
    "\n",
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Simple, end-to-end, LeNet-5-like convolutional MNIST model example.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'\n",
    "WORK_DIRECTORY = 'data'\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 10\n",
    "VALIDATION_SIZE = 5000  # Size of the validation set.\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "EVAL_BATCH_SIZE = 64\n",
    "EVAL_FREQUENCY = 100  # Number of steps between evaluations.\n",
    "\n",
    "\n",
    "def download(filename):\n",
    "  \"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n",
    "  if not os.path.exists(WORK_DIRECTORY):\n",
    "    os.makedirs(WORK_DIRECTORY)\n",
    "  filepath = os.path.join(WORK_DIRECTORY, filename)\n",
    "  if not os.path.exists(filepath):\n",
    "    filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename,\n",
    "                                             filepath)\n",
    "    size = os.stat(filepath).st_size\n",
    "    print('Successfully downloaded', filename, size, 'bytes.')\n",
    "  return filepath\n",
    "\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "  \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "\n",
    "  Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "  \"\"\"\n",
    "  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    bytestream.read(16)\n",
    "    buf = bytestream.read(\n",
    "        IMAGE_SIZE * IMAGE_SIZE * num_images * NUM_CHANNELS)\n",
    "    data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(\n",
    "        numpy.float32)\n",
    "    # The original data consists of pixels ranging from 0-255.\n",
    "    # Center the data to have mean zero, and unit range.\n",
    "    data = (data - (255/2.0))/255 \n",
    "    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE,\n",
    "                        NUM_CHANNELS)\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "  \"\"\"Extract the labels into a vector of int64 label IDs.\"\"\"\n",
    "  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    # Discard header.\n",
    "    bytestream.read(8)\n",
    "    # Read bytes for labels.\n",
    "    buf = bytestream.read(num_images)\n",
    "    labels = numpy.frombuffer(buf, dtype=numpy.uint8).astype(\n",
    "        numpy.int64)\n",
    "  return labels\n",
    "\n",
    "\n",
    "def error_rate(predictions, labels):\n",
    "  \"\"\"Return the error rate based on dense predictions and sparse labels.\"\"\"\n",
    "  return 100.0 - (\n",
    "      100.0 *\n",
    "      numpy.sum(numpy.argmax(predictions, 1) == labels) /\n",
    "      predictions.shape[0])\n",
    "\n",
    "# We will replicate the model structure for the training subgraph, as\n",
    "# well as the evaluation subgraphs, while sharing the trainable\n",
    "# parameters.\n",
    "def model(data, train=False):\n",
    "  \"\"\"The Model definition.\"\"\"\n",
    "  # 2D convolution, with 'SAME' padding (i.e. the output feature map\n",
    "  # has the same size as the input). Note that {strides} is a 4D array\n",
    "  # whose shape matches the data layout: [image index, y, x, depth].\n",
    "  conv = tf.nn.conv2d(data,\n",
    "                      conv1_weights,\n",
    "                      strides=[1, 1, 1, 1],\n",
    "                      padding='SAME')\n",
    "  # Bias and rectified linear non-linearity.\n",
    "  relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "  # Max pooling. The kernel size spec {ksize} also follows the layout\n",
    "  # of the data. Here we have a pooling window of 2, and a stride of\n",
    "  # 2.\n",
    "  pool = tf.nn.max_pool(relu,\n",
    "                        ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1],\n",
    "                        padding='SAME')\n",
    "  conv = tf.nn.conv2d(pool,\n",
    "                      conv2_weights,\n",
    "                      strides=[1, 1, 1, 1],\n",
    "                      padding='SAME')\n",
    "  relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "  pool = tf.nn.max_pool(relu,\n",
    "                        ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1],\n",
    "                        padding='SAME')\n",
    "  # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "  # fully connected layers.\n",
    "  pool_shape = pool.get_shape().as_list()\n",
    "  reshape = tf.reshape(\n",
    "      pool,\n",
    "      [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "  # Fully connected layer. Note that the '+' operation automatically\n",
    "  # broadcasts the biases.\n",
    "  hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "  # Add a 50% dropout during training only. Dropout also scales\n",
    "  # activations such that no rescaling is needed at evaluation time.\n",
    "  if train:\n",
    "    hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "  return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "\n",
    "\n",
    "\n",
    "# Get the data.\n",
    "train_data_filename = download('train-images-idx3-ubyte.gz')\n",
    "train_labels_filename = download('train-labels-idx1-ubyte.gz')\n",
    "test_data_filename = download('t10k-images-idx3-ubyte.gz')\n",
    "test_labels_filename = download('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "# Extract it into numpy arrays.\n",
    "train_data = extract_data(train_data_filename, 60000)\n",
    "train_labels = extract_labels(train_labels_filename, 60000)\n",
    "test_data = extract_data(test_data_filename, 10000)\n",
    "test_labels = extract_labels(test_labels_filename, 10000)\n",
    "\n",
    "# Generate a validation set.\n",
    "validation_data = train_data[:VALIDATION_SIZE, ...]\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "train_data = train_data[VALIDATION_SIZE:, ...]\n",
    "train_labels = train_labels[VALIDATION_SIZE:]\n",
    "\n",
    "num_epochs = NUM_EPOCHS\n",
    "train_size = train_labels.shape[0]\n",
    "\n",
    "# This is where training samples and labels are fed to the graph.\n",
    "# These placeholder nodes will be fed a batch of training data at each\n",
    "# training step using the {feed_dict} argument to the Run() call below.\n",
    "train_data_node = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.int64, shape=(BATCH_SIZE,))\n",
    "eval_data = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "\n",
    "# The variables below hold all the trainable weights. They are passed\n",
    "# an initial value which will be assigned when we call:\n",
    "# {tf.global_variables_initializer().run()}\n",
    "conv1_weights = tf.Variable(\n",
    "    # 5x5 filter, depth 32.\n",
    "    tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  \n",
    "                        stddev=0.1,\n",
    "                        seed=SEED, dtype=tf.float32))\n",
    "conv1_biases = tf.Variable(tf.zeros([32], dtype=tf.float32))\n",
    "conv2_weights = tf.Variable(tf.truncated_normal(\n",
    "    [5, 5, 32, 64], stddev=0.1,\n",
    "    seed=SEED, dtype=tf.float32))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64],\n",
    "                           dtype=tf.float32))\n",
    "fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "    tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n",
    "                        stddev=0.1,\n",
    "                        seed=SEED,\n",
    "                        dtype=tf.float32))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512],\n",
    "                         dtype=tf.float32))\n",
    "fc2_weights = tf.Variable(tf.truncated_normal([512, NUM_LABELS],\n",
    "                                              stddev=0.1,\n",
    "                                              seed=SEED,\n",
    "                                              dtype=tf.float32))\n",
    "fc2_biases = tf.Variable(tf.constant(\n",
    "    0.1, shape=[NUM_LABELS], dtype=tf.float32))\n",
    "\n",
    "# Training computation: logits + cross-entropy loss.\n",
    "logits = model(train_data_node, True)\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=train_labels_node, logits=logits))\n",
    "\n",
    "# L2 regularization for the fully connected parameters.\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights)\n",
    "                + tf.nn.l2_loss(fc1_biases)\n",
    "                + tf.nn.l2_loss(fc2_weights)\n",
    "                + tf.nn.l2_loss(fc2_biases))\n",
    "# Add the regularization term to the loss.\n",
    "loss += 5e-4 * regularizers\n",
    "\n",
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "batch = tf.Variable(0, dtype=tf.float32)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    0.01,                # Base learning rate.\n",
    "    batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "    train_size,          # Decay step.\n",
    "    0.95,                # Decay rate.\n",
    "    staircase=True)\n",
    "# Use simple momentum for the optimization.\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                       0.9).minimize(loss,\n",
    "                                                     global_step=batch)\n",
    "\n",
    "# Predictions for the current training minibatch.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Predictions for the test and validation, which we'll compute less\n",
    "# often.\n",
    "eval_prediction = tf.nn.softmax(model(eval_data))\n",
    "\n",
    "# Small utility function to evaluate a dataset by feeding batches of\n",
    "# data to {eval_data} and pulling the results from {eval_predictions}.\n",
    "# Saves memory and enables this to run on smaller GPUs.\n",
    "def eval_in_batches(data, sess):\n",
    "  \"\"\"Get predictions for a dataset by running it in small batches.\"\"\"\n",
    "  size = data.shape[0]\n",
    "  if size < EVAL_BATCH_SIZE:\n",
    "    raise ValueError(\"batch size for evals larger than dataset: %d\"\n",
    "                     % size)\n",
    "  predictions = numpy.ndarray(shape=(size, NUM_LABELS),\n",
    "                              dtype=numpy.float32)\n",
    "  for begin in xrange(0, size, EVAL_BATCH_SIZE):\n",
    "    end = begin + EVAL_BATCH_SIZE\n",
    "    if end <= size:\n",
    "      predictions[begin:end, :] = sess.run(\n",
    "          eval_prediction,\n",
    "          feed_dict={eval_data: data[begin:end, ...]})\n",
    "    else:\n",
    "      batch_predictions = sess.run(\n",
    "          eval_prediction,\n",
    "          feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n",
    "      predictions[begin:, :] = batch_predictions[begin - size:, :]\n",
    "  return predictions\n",
    "\n",
    "# Create a local session to run the training.\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "  # Run all the initializers to prepare the trainable parameters.\n",
    "  tf.global_variables_initializer().run()\n",
    "  # Loop through training steps.\n",
    "  for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):\n",
    "    # Compute the offset of the current minibatch in the data.\n",
    "    # Note that we could use better randomization across epochs.\n",
    "    offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "    batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n",
    "    batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "    # This dictionary maps the batch data (as a numpy array) to the\n",
    "    # node in the graph it should be fed to.\n",
    "    feed_dict = {train_data_node: batch_data,\n",
    "                 train_labels_node: batch_labels}\n",
    "    # Run the optimizer to update weights.\n",
    "    sess.run(optimizer, feed_dict=feed_dict)\n",
    "    # print some extra information once reach the evaluation frequency\n",
    "    if step % EVAL_FREQUENCY == 0:\n",
    "      # fetch some extra nodes' data\n",
    "      l, lr, predictions = sess.run([loss, learning_rate,\n",
    "                                     train_prediction],\n",
    "                                    feed_dict=feed_dict)\n",
    "      elapsed_time = time.time() - start_time\n",
    "      start_time = time.time()\n",
    "      print('Step %d (epoch %.2f), %.1f ms' %\n",
    "            (step, float(step) * BATCH_SIZE / train_size,\n",
    "             1000 * elapsed_time / EVAL_FREQUENCY))\n",
    "      print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n",
    "      print('Minibatch error: %.1f%%'\n",
    "            % error_rate(predictions, batch_labels))\n",
    "      print('Validation error: %.1f%%' % error_rate(\n",
    "          eval_in_batches(validation_data, sess), validation_labels))\n",
    "      sys.stdout.flush()\n",
    "  # Finally print the result!\n",
    "  test_error = error_rate(eval_in_batches(test_data, sess),\n",
    "                          test_labels)\n",
    "  print('Test error: %.1f%%' % test_error)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env01",
   "language": "python",
   "name": "env01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
